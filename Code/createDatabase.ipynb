{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import contextlib\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import logging\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import json\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "# pm4py imports\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_replay\n",
    "\n",
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristic_miner\n",
    "from pm4py.algo.discovery.ilp import algorithm as ilp_miner\n",
    "from pm4py.algo.evaluation import algorithm as evaluation\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "import calcEventLogPs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_folder = \"/home/jupyter-benjamin.andrick-3cf07/test/logs/inductive_logs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Directory for logs\n",
    "log_dir = \"/home/jupyter-benjamin.andrick-3cf07/test/logfiles\"\n",
    "# Setup logging with the new path\n",
    "log_filename = os.path.join(log_dir, f\"process_mining_{datetime.now().strftime('%Y%m%d_%H%M%S')}conformance_tag.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    level=logging.INFO,  # Use a valid logging level here\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logging.info('NEW RUN\\n\\n\\n------------------------------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_process_models(log):\n",
    " \n",
    "    results = {}\n",
    "    \n",
    "    # ILP Miner\n",
    "    #logging.info(\"Applying ILP Miner - Classic Variant...\")\n",
    "    ilp_net, ilp_im, ilp_fm = ilp_miner.apply(log)\n",
    "    results['ILP Miner'] = (ilp_net, ilp_im, ilp_fm)\n",
    "    \n",
    "    # Heuristic Miner - Classic\n",
    "    parameters = {\n",
    "        heuristic_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.75\n",
    "    }\n",
    "    #logging.info(\"Applying Heuristic Miner - Classic Variant...\")\n",
    "    heur_net, heur_im, heur_fm = heuristic_miner.apply(\n",
    "        log,\n",
    "        parameters=parameters,\n",
    "        variant=heuristic_miner.Variants.CLASSIC\n",
    "    )\n",
    "    results['Heuristic Miner - Classic'] = (heur_net, heur_im, heur_fm)\n",
    "    \n",
    "    # Heuristic Miner - Plus\n",
    "    parameters_plus = {\n",
    "        heuristic_miner.Variants.PLUSPLUS.value.Parameters.DEPENDENCY_THRESH: 0.75\n",
    "    }\n",
    "    #logging.info(\"Applying Heuristic Miner - Plusplus Variant...\")\n",
    "    heur_plus_net, heur_plus_im, heur_plus_fm = heuristic_miner.apply(\n",
    "        log,\n",
    "        parameters=parameters_plus,\n",
    "        variant=heuristic_miner.Variants.PLUSPLUS\n",
    "    )\n",
    "    results['Heuristic Miner - Plus'] = (heur_plus_net, heur_plus_im, heur_plus_fm)\n",
    "    \n",
    "    # Alpha Miner - Classic\n",
    "    #logging.info(\"Applying Alpha Miner - Classic Variant...\")\n",
    "    alpha_net, alpha_im, alpha_fm = alpha_miner.apply(\n",
    "        log,\n",
    "        variant=alpha_miner.Variants.ALPHA_VERSION_CLASSIC\n",
    "    )\n",
    "    results['Alpha Miner - Classic'] = (alpha_net, alpha_im, alpha_fm)\n",
    "    \n",
    "    # Alpha Miner - Plus\n",
    "    #logging.info(\"Applying Alpha Miner - Plus Variant...\")\n",
    "    #alpha_plus_net, alpha_plus_im, alpha_plus_fm = alpha_miner.apply(\n",
    "    #    log,\n",
    "   #     variant=alpha_miner.Variants.ALPHA_VERSION_PLUS\n",
    "    #)\n",
    "    #results['Alpha Miner - Plus'] = (alpha_plus_net, alpha_plus_im, alpha_plus_fm)\n",
    "    #logging.info('mining done in def')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_conformance_checking(log, models):\n",
    " \n",
    "    results = {}\n",
    "    \n",
    "    for algorithm_name, (net, initial_marking, final_marking) in models.items():\n",
    "        #logging.info(f\"\\n=== Evaluating {algorithm_name} ===\")\n",
    "        try:\n",
    "            # Use the evaluation algorithm which provides comprehensive metrics\n",
    "            metrics = evaluation.apply(\n",
    "                log,\n",
    "                net,\n",
    "                initial_marking,\n",
    "                final_marking\n",
    "            )\n",
    "            \n",
    "            # Flatten the fitness metrics and combine with other metrics\n",
    "            flattened_metrics = {\n",
    "                'perc_fit_traces': metrics['fitness']['perc_fit_traces'],\n",
    "                'average_trace_fitness': metrics['fitness']['average_trace_fitness'],\n",
    "                'log_fitness': metrics['fitness']['log_fitness'],\n",
    "                'percentage_of_fitting_traces': metrics['fitness']['percentage_of_fitting_traces'],\n",
    "                'precision': metrics['precision'],\n",
    "                'generalization': metrics['generalization'],\n",
    "                'simplicity': metrics['simplicity'],\n",
    "                'metricsAverageWeight': metrics.get('metricsAverageWeight', None),\n",
    "                'fscore': metrics.get('fscore', None)\n",
    "            }\n",
    "            \n",
    "            results[algorithm_name] = flattened_metrics\n",
    "            \n",
    "          \n",
    "            \n",
    "        except Exception as e:\n",
    "            #logging.info(f\"Error evaluating {algorithm_name}: {str(e)}\")\n",
    "            results[algorithm_name] = {\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xes_file_with_timeout(file_path):\n",
    "    #logging.info(f\"Processing file: {file_path}\") \n",
    "    # Import the XES log\n",
    "    log = xes_importer.apply(file_path)\n",
    "    \n",
    "    log_name = os.path.basename(file_path)\n",
    "    df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "    \n",
    "    #Step 3: Add artificial timestamps if missing\n",
    "    if 'time:timestamp' not in df.columns:\n",
    "       df['time:timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='s')\n",
    "    \n",
    "    #Step 4: Convert back to event log\n",
    "    log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "    #logging.info(\"Calculating Event Log Properties...\")    \n",
    "    #props = calcEventLogPs.calculate_event_log_ps(log)\n",
    "    #logging.info(\"Performing Conformance Checking...\")\n",
    "    res = discover_process_models(log)\n",
    "    #logging.info(\"Mining done nach def\")\n",
    "    conformance_results = perform_conformance_checking(log, res)\n",
    "    #logging.info(\"Merging Results...\")\n",
    "    merged_results = {\n",
    "        log_name: {\n",
    "            \"conformance\": conformance_results\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return merged_results\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def run_with_timeout(file_path):\n",
    "    def thread_function(future, file_path):\n",
    "        try:\n",
    "            result = process_xes_file_with_timeout(file_path)\n",
    "            future.set_result(result)\n",
    "        except Exception as e:\n",
    "            future.set_exception(e)\n",
    "\n",
    "    future = concurrent.futures.Future()\n",
    "    thread = threading.Thread(target=thread_function, args=(future, file_path))\n",
    "    # Set thread as daemon so it will be terminated when main thread exits\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "\n",
    "    try:\n",
    "        return future.result(timeout=60*12)  \n",
    "    except concurrent.futures.TimeoutError:\n",
    "        logging.info(f\"Processing timed out for {os.path.basename(file_path)}\")\n",
    "        # Add the file to timeout_fails.txt\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        timeout_file = os.path.join(folder_path, 'timeout_fails.txt')\n",
    "        with open(timeout_file, 'a') as f:\n",
    "            f.write(os.path.basename(file_path) + '\\n')\n",
    "        return None\n",
    "    finally:\n",
    "        # More aggressive thread termination\n",
    "        try:\n",
    "            thread.join(timeout=1)  # Give the thread 1 second to finish\n",
    "            if thread.is_alive():\n",
    "                logging.info(f\"Thread for {os.path.basename(file_path)} did not terminate gracefully\")\n",
    "                # Attempt to raise exception in thread to force termination\n",
    "                import ctypes\n",
    "                thread_id = thread.ident\n",
    "                if thread_id is not None:\n",
    "                    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(\n",
    "                        ctypes.c_long(thread_id), \n",
    "                        ctypes.py_object(SystemExit)\n",
    "                    )\n",
    "                    if res > 1:\n",
    "                        # If it went wrong, cancel it\n",
    "                        ctypes.pythonapi.PyThreadState_SetAsyncExc(\n",
    "                            ctypes.c_long(thread_id), \n",
    "                            None\n",
    "                        )\n",
    "                # Give it a moment to terminate\n",
    "                thread.join(0.1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while terminating thread: {str(e)}\")\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main multiple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder 1 of 5: standard\n",
      "Folder progress: 20.00%\n",
      "Processing folder 2 of 5: inductive_logs\n",
      "Folder progress: 40.00%\n",
      "Processing folder 3 of 5: variants_coverage_filtered\n",
      "Folder progress: 60.00%\n",
      "Processing folder 4 of 5: variants_top_k_filtered\n",
      "Processing file pdc_2020_0111101.xes_variants_top_k.xes, 0 of 543 in folder variants_top_k_filtered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 800/800 [00:00<00:00, 6243.59it/s]\n",
      "discovering Petri net using ILP miner, completed causal relations :: 100%|██████████| 82/82 [00:10<00:00,  7.48it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 662/662 [00:01<00:00, 491.02it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 8820/8820 [00:32<00:00, 273.29it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 662/662 [00:01<00:00, 538.27it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 8820/8820 [00:05<00:00, 1687.42it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 662/662 [00:02<00:00, 326.65it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 8820/8820 [00:05<00:00, 1574.70it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 662/662 [00:00<00:00, 1336.15it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 8820/8820 [00:01<00:00, 7769.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file activitylog_uci_detailed_labour.xes_variants_top_k.xes, 1 of 543 in folder variants_top_k_filtered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 20/20 [00:00<00:00, 1273.95it/s]\n",
      "discovering Petri net using ILP miner, completed causal relations :: 100%|██████████| 32/32 [00:00<00:00, 48.68it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 20/20 [00:00<00:00, 294.18it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 987/987 [00:02<00:00, 382.92it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 20/20 [00:00<00:00, 225.79it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 987/987 [00:00<00:00, 1251.84it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 20/20 [00:00<00:00, 486.22it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 987/987 [00:00<00:00, 10036.39it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 20/20 [00:00<00:00, 1057.05it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 987/987 [00:00<00:00, 6216.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file pdc2021_021000.xes_variants_top_k.xes, 2 of 543 in folder variants_top_k_filtered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 200/200 [00:00<00:00, 2874.21it/s]\n",
      "discovering Petri net using ILP miner, completed causal relations ::  84%|████████▍ | 109/130 [00:30<00:07,  3.00it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m     59\u001b[0m base_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jupyter-maximilian.hoelper-3cf07/test/logs\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your folder path\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m, in \u001b[0;36mprocess_folders\u001b[0;34m(base_folder)\u001b[0m\n\u001b[1;32m     38\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     fitness \u001b[38;5;241m=\u001b[39m \u001b[43mrun_with_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fitness \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         existing_results\u001b[38;5;241m.\u001b[39mappend(fitness)\n",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m, in \u001b[0;36mrun_with_timeout\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mTimeoutError:\n\u001b[1;32m     49\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing timed out for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.13/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.13/threading.py:363\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 363\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "discovering Petri net using ILP miner, completed causal relations :: 100%|██████████| 130/130 [00:36<00:00,  3.57it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 200/200 [00:01<00:00, 136.93it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 5472/5472 [00:44<00:00, 123.20it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 200/200 [00:01<00:00, 180.84it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 5472/5472 [00:08<00:00, 656.67it/s] \n",
      "replaying log with TBR, completed traces :: 100%|██████████| 200/200 [00:01<00:00, 129.33it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 5472/5472 [00:07<00:00, 740.22it/s] \n",
      "replaying log with TBR, completed traces :: 100%|██████████| 200/200 [00:00<00:00, 311.39it/s]\n",
      "replaying log with TBR, completed traces :: 100%|██████████| 5472/5472 [00:01<00:00, 5459.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# ... existing imports and setup ...\n",
    "\n",
    "def process_folders(base_folder):\n",
    "    fitness_results = []\n",
    "    total_folders = [f for f in os.listdir(base_folder) \n",
    "                    if os.path.isdir(os.path.join(base_folder, f)) and not f.startswith('.')]\n",
    "    total_count = len(total_folders)\n",
    "    folder_count = 0\n",
    "\n",
    "    for folder in total_folders:\n",
    "        folder_path = os.path.join(base_folder, folder)\n",
    "        print(f\"Processing folder {folder_count + 1} of {total_count}: {folder}\")\n",
    "        \n",
    "        # Initialize or load existing JSON file for this folder\n",
    "        results_file = os.path.join(folder_path, 'conformance_results.json')\n",
    "        if os.path.exists(results_file):\n",
    "            with open(results_file, 'r') as f:\n",
    "                try:\n",
    "                    existing_results = json.load(f)\n",
    "                    processed_files = {list(result.keys())[0] for result in existing_results if result}\n",
    "                except json.JSONDecodeError:\n",
    "                    existing_results = []\n",
    "                    processed_files = set()\n",
    "        else:\n",
    "            existing_results = []\n",
    "            processed_files = set()\n",
    "\n",
    "        # Process XES files in the folder\n",
    "        files = [f for f in os.listdir(folder_path) \n",
    "                if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.xes')]\n",
    "        file_count = 0\n",
    "        for file in files:\n",
    "            if file in processed_files:\n",
    "                #print(f\"File '{file}' already processed. Skipping...\")\n",
    "                file_count += 1\n",
    "                continue\n",
    "            print(f\"Processing file {file}, {file_count} of {len(files)} in folder {folder}\")\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                fitness = run_with_timeout(file_path)\n",
    "                \n",
    "                if fitness is not None:\n",
    "                    existing_results.append(fitness)\n",
    "                    # Write results immediately after each file\n",
    "                    with open(results_file, 'w') as f:\n",
    "                        json.dump(existing_results, f, indent=4)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file}: {str(e)}\")\n",
    "            file_count += 1\n",
    "\n",
    "        folder_count += 1\n",
    "        print(f\"Folder progress: {(folder_count / total_count) * 100:.2f}%\")\n",
    "        #logging.info(f\"Completed folder {folder} ({folder_count}/{total_count})\")\n",
    "\n",
    "    return fitness_results\n",
    "\n",
    "# Usage\n",
    "base_folder = \"/home/jupyter-benjamin.andrick-3cf07/test/logs\"  # Replace with your folder path\n",
    "results = process_folders(base_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main singular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datei 'inductive_pdc2024_020010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_BPI Challenge 2017 - Offer log.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_5.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_100010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_101110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_BPI_Challenge_2013_incidents.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_SEPSIS.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_edited_hh102_labour.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_101000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_001010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_edited_hh110_weekends.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_000111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_000110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_111101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_111111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_101111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_001000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_100110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_111110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_000010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_BPIC15_1f.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_6.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_edited_hh110_labour.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_011101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_ETM_Configuration3.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_020011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_011111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_001110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_121101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_4.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_edited_hh104_labour.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_110101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_10.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_001111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_3.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_021000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_4.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_nasa-cev-1-10-splitted.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_000000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_BPIC15_5f.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_011100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_120000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_110001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_101100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_020101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_020000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_110110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_011110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_100011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_001011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_2.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_ETM_Configuration2.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_101010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_001001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_9.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_5.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_011000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_010000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_10.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_121100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_activitylog_uci_detailed_weekends.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_100001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_120110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_000011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_DomesticDeclarations.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_BPIC13_i.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_121011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_110000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_010010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_2.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_021001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_BPIC14_f.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_100111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_7.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_111000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_101001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_110011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_020001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_000101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_021101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_011010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_101011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_7.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_021100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_120011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_111010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_020110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_121111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_edited_hh104_weekends.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_BPIC12.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_111100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_100101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_RTFMP.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_001100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_120010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_000001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_021010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_021111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_100100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_110111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_120001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_120101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_1.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_110010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_121010.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_021011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_120100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_010001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_010011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_000100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_BPIC15_2f.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_010100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_111011.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_121110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_010111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_111001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_020111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_edited_hh102_weekends.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_BPIC15_4f.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_100000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_110100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_1.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_121000.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_8.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_3.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_001101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_JUnit 4.12 Software Event Log.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_121001.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2016_6.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_120111.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_010110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_010101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_101101.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_PrepaidTravelCost.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_activitylog_uci_detailed_labour.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_9.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_021110.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc2024_020100.xes' wurde bereits verarbeitet. Überspringe...\n",
      "Datei 'inductive_pdc_2017_8.xes' wurde bereits verarbeitet. Überspringe...\n"
     ]
    }
   ],
   "source": [
    "fitness_results = []\n",
    "count = 0\n",
    "\n",
    "dateien = [f for f in os.listdir(log_folder) if os.path.isfile(os.path.join(log_folder, f))]\n",
    "gesamtanzahl = len(dateien)\n",
    "\n",
    "# Initialize or load existing JSON file\n",
    "results_file = os.path.join(log_folder, 'conformance_results.json')\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        try:\n",
    "            existing_results = json.load(f)\n",
    "            # Extract already processed filenames from existing results\n",
    "            processed_files = {list(result.keys())[0] for result in existing_results if result}\n",
    "        except json.JSONDecodeError:\n",
    "            existing_results = []\n",
    "            processed_files = set()\n",
    "else:\n",
    "    existing_results = []\n",
    "    processed_files = set()\n",
    "\n",
    "for datei in dateien:\n",
    "    if not datei.endswith(\".xes\"):\n",
    "        continue\n",
    "    if datei in processed_files:\n",
    "        print(f\"Datei '{datei}' wurde bereits verarbeitet. Überspringe...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Verarbeite Datei {count} von {gesamtanzahl}: {datei}\")\n",
    "    dateipfad = os.path.join(log_folder, datei)\n",
    "\n",
    "    try:\n",
    "        with open(dateipfad, 'r') as f:\n",
    "            fitness = run_with_timeout(dateipfad)\n",
    "            \n",
    "            # Append new result to existing results and write immediately\n",
    "            if fitness is not None:\n",
    "                existing_results.append(fitness)\n",
    "                with open(results_file, 'w') as f:\n",
    "                    json.dump(existing_results, f, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Laden von {datei}: {e}\")\n",
    "\n",
    "    count += 1\n",
    "    print(f\"{(count / gesamtanzahl) * 100:.2f}% abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "replaying log with TBR, completed traces :: 100%|██████████| 4911/4911 [01:29<00:00, 54.83it/s] \n",
      "replaying log with TBR, completed traces :: 100%|██████████| 764/764 [00:12<00:00, 61.32it/s] \n",
      "replaying log with TBR, completed traces :: 100%|██████████| 4911/4911 [00:25<00:00, 190.02it/s] \n",
      "replaying log with TBR, completed traces :: 100%|██████████| 764/764 [00:14<00:00, 52.37it/s] \n",
      "replaying log with TBR, completed traces :: 100%|██████████| 4911/4911 [01:09<00:00, 70.18it/s] \n"
     ]
    }
   ],
   "source": [
    "#calc_features = False\n",
    "# Folder containing XES files\n",
    "#log_folder = r'/Volumes/NO NAME/Event Logs/Test_Logs'\n",
    "#log_folder = '/Users/benjaminandrick/Documents/Studium/Semester 7/Bachelorarbeit/Code/Logs'\n",
    "      # You might want to add more aggressive termination here if necessary\n",
    "# Process all XES files in the folder\n",
    "fitness_results = []\n",
    "count = 0\n",
    "geladene_dateien_pfad = os.path.join(log_folder, 'geladene_dateien.txt')\n",
    "if os.path.exists(geladene_dateien_pfad):\n",
    "    with open(geladene_dateien_pfad, 'r') as f:\n",
    "        bereits_geladene_dateien = set(line.strip() for line in f)\n",
    "else:\n",
    "    bereits_geladene_dateien = set()\n",
    "\n",
    "dateien = [f for f in os.listdir(log_folder) if os.path.isfile(os.path.join(log_folder, f))]\n",
    "gesamtanzahl = len(dateien)\n",
    "\n",
    "# Initialize or load existing JSON file\n",
    "results_file = os.path.join(log_folder, 'conformance_results.json')\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        try:\n",
    "            existing_results = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            existing_results = []\n",
    "else:\n",
    "    existing_results = []\n",
    "\n",
    "for datei in dateien:\n",
    "    if not datei.endswith(\".xes\"):\n",
    "        continue\n",
    "    if datei in bereits_geladene_dateien:\n",
    "        print(f\"Datei '{datei}' wurde bereits geladen. Überspringe...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Verarbeite Datei {count} von {gesamtanzahl}: {datei}\")\n",
    "    dateipfad = os.path.join(log_folder, datei)\n",
    "\n",
    "    try:\n",
    "        with open(dateipfad, 'r') as f:\n",
    "            fitness = run_with_timeout(dateipfad)\n",
    "            \n",
    "            # Append new result to existing results and write immediately\n",
    "            if fitness is not None:\n",
    "                existing_results.append(fitness)\n",
    "                with open(results_file, 'w') as f:\n",
    "                    json.dump(existing_results, f, indent=4)\n",
    "\n",
    "        # Mark file as processed\n",
    "        with open(geladene_dateien_pfad, 'a') as f:\n",
    "            f.write(datei + '\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Laden von {datei}: {e}\")\n",
    "\n",
    "    count += 1\n",
    "    print(f\"{(count / gesamtanzahl) * 100:.2f}% abgeschlossen\")\n",
    "    logging.info(f\"{(count / gesamtanzahl) * 100:.2f}% abgeschlossen\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = os.path.join(log_folder, f\"fitness_results_inductive.json\")\n",
    "with open(output, 'w') as f:\n",
    "    json.dump(fitness_results, f, indent = 4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the output file path (using a consistent filename)\n",
    "output = os.path.join(log_folder, 'fitness_results.json')\n",
    "\n",
    "# Initialize a list to hold all fitness results\n",
    "all_fitness_results = []\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(output):\n",
    "    # Read the existing data from the file\n",
    "    with open(output, 'r') as f:\n",
    "        try:\n",
    "            all_fitness_results = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle empty or invalid JSON file\n",
    "            all_fitness_results = []\n",
    "\n",
    "# Append the new fitness results to the list\n",
    "all_fitness_results.extend(fitness_results)\n",
    "\n",
    "# Write the updated list back to the file\n",
    "with open(output, 'w') as f:\n",
    "    json.dump(all_fitness_results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m geladene_dateien_pfad \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeladene_dateien.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Unverarbeitete Dateien abrufen (nur .xes-Dateien)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m unprocessed_files \u001b[38;5;241m=\u001b[39m get_unprocessed_files(\u001b[43mlog_folder\u001b[49m, geladene_dateien_pfad, extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverarbeitete Dateien:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m unprocessed_files:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_folder' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_unprocessed_files(folder_path, processed_files_path, extension=None):\n",
    "    \"\"\"\n",
    "    Gibt eine Liste von Dateien im Ordner zurück, die nicht in processed_files_path aufgeführt sind.\n",
    "    Wenn eine Erweiterung angegeben ist, werden nur Dateien mit dieser Erweiterung berücksichtigt.\n",
    "    \"\"\"\n",
    "    # Alle Dateien im Ordner abrufen\n",
    "    all_files = set(os.listdir(folder_path))\n",
    "    \n",
    "    # Optional: Dateien nach Erweiterung filtern\n",
    "    if extension:\n",
    "        all_files = {f for f in all_files if f.endswith(extension)}\n",
    "    \n",
    "    # Set mit bereits verarbeiteten Dateien initialisieren\n",
    "    if os.path.exists(processed_files_path):\n",
    "        with open(processed_files_path, 'r') as f:\n",
    "            processed_files = set(line.strip() for line in f)\n",
    "    else:\n",
    "        processed_files = set()\n",
    "    \n",
    "    # Unverarbeitete Dateien ermitteln\n",
    "    unprocessed_files = all_files - processed_files\n",
    "    \n",
    "    return list(unprocessed_files)\n",
    "    \n",
    "geladene_dateien_pfad = 'geladene_dateien.txt'\n",
    "\n",
    "# Unverarbeitete Dateien abrufen (nur .xes-Dateien)\n",
    "unprocessed_files = get_unprocessed_files(log_folder, geladene_dateien_pfad, extension='.xes')\n",
    "\n",
    "logging.info(\"Unverarbeitete Dateien:\")\n",
    "for filename in unprocessed_files:\n",
    "    logging.info(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ERROR - Fehler beim Laden von Road_Traffic_Fine_Management_Process.xes: Attribute value redefined, line 2498204, column 98 (Road_Traffic_Fine_Management_Process.xes, line 2498204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'BPIC12.xes': {'conformance': {'ILP Miner': {'perc_fit_traces': 100.0,\n",
       "     'average_trace_fitness': 1.0,\n",
       "     'log_fitness': 1.0,\n",
       "     'percentage_of_fitting_traces': 100.0,\n",
       "     'precision': 0.1842215110616391,\n",
       "     'generalization': 0.6892322645020896,\n",
       "     'simplicity': 0.09298998569384835,\n",
       "     'metricsAverageWeight': 0.4916109403143943,\n",
       "     'fscore': 0.31112677711197273},\n",
       "    'Heuristic Miner - Classic': {'perc_fit_traces': 11.0,\n",
       "     'average_trace_fitness': 0.8483540326185368,\n",
       "     'log_fitness': 0.8569110264843345,\n",
       "     'percentage_of_fitting_traces': 11.0,\n",
       "     'precision': 0.7678244972577697,\n",
       "     'generalization': 0.5266178800130592,\n",
       "     'simplicity': 0.5212765957446809,\n",
       "     'metricsAverageWeight': 0.6681574998749611,\n",
       "     'fscore': 0.8099253921519004},\n",
       "    'Heuristic Miner - Plus': {'perc_fit_traces': 37.0,\n",
       "     'average_trace_fitness': 0.8169760209957636,\n",
       "     'log_fitness': 0.7223942153424974,\n",
       "     'percentage_of_fitting_traces': 37.0,\n",
       "     'precision': 1.0,\n",
       "     'generalization': 0.4023502762928032,\n",
       "     'simplicity': 0.5094339622641509,\n",
       "     'metricsAverageWeight': 0.6585446134748629,\n",
       "     'fscore': 0.8388256403878477},\n",
       "    'Alpha Miner - Classic': {'perc_fit_traces': 0.0,\n",
       "     'average_trace_fitness': 0.7206549332017596,\n",
       "     'log_fitness': 0.6615438298845181,\n",
       "     'percentage_of_fitting_traces': 0.0,\n",
       "     'precision': 0.18811556139198948,\n",
       "     'generalization': 0.678693877727194,\n",
       "     'simplicity': 0.9393939393939394,\n",
       "     'metricsAverageWeight': 0.6169368020994103,\n",
       "     'fscore': 0.2929331217234408},\n",
       "    'Alpha Miner - Plus': {'perc_fit_traces': 0.0,\n",
       "     'average_trace_fitness': 0.530678284084672,\n",
       "     'log_fitness': 0.5902455770264086,\n",
       "     'percentage_of_fitting_traces': 0.0,\n",
       "     'precision': 0.20443904976590943,\n",
       "     'generalization': 0.6827702521023031,\n",
       "     'simplicity': 0.7446808510638298,\n",
       "     'metricsAverageWeight': 0.5555339324896127,\n",
       "     'fscore': 0.30369090033333546}},\n",
       "   'properties': {'Number of Events': 619,\n",
       "    'ATS': 4.62,\n",
       "    'Number of Traces': 100,\n",
       "    'Distinct Events': 20,\n",
       "    'Distinct Traces': 58,\n",
       "    'Distinct Start Events': 1,\n",
       "    'Distinct End Events': 12,\n",
       "    'Average Trace Length': 6.19,\n",
       "    'Max Trace Length': 33,\n",
       "    'Min Trace Length': 2,\n",
       "    'Event Density': 0.7463651050080775,\n",
       "    'Absolute Trace Coverage': 38,\n",
       "    'Relative Trace Coverage': 0.38,\n",
       "    'Structure': 0.7525,\n",
       "    'Level of Detail': 4.62,\n",
       "    'Traces with Self-loops': 33,\n",
       "    'Total Self-loops': 88,\n",
       "    'Average Self-loop Size': 2.442622950819672}}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
