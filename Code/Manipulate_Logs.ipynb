{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pm4py\n",
    "from pm4py.filtering import filter_case_size, filter_variants_top_k, filter_variants_by_coverage_percentage\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pm4py.statistics.variants.log import get as variants_module\n",
    "from pm4py.algo.filtering.log.variants import variants_filter\n",
    "import csv\n",
    "import calcEventLogPs\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_path = '/Users/benjaminandrick/Documents/Studium/Semester 7/Bachelorarbeit/Code/Test_Logs'\n",
    "log_path = '/home/jupyter-benjamin.andrick-3cf07/test/logs/standard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_filter_variants_by_coverage(log):\n",
    "    \"\"\"\n",
    "    Dynamically filters variants based on their cumulative coverage in the log.\n",
    "    \n",
    "    Parameters:\n",
    "    - log: The event log to filter.\n",
    "\n",
    "    Returns:\n",
    "    - filtered_log: The log filtered to include only the most significant variants.\n",
    "    \"\"\"\n",
    "    # Get all variants and their frequency\n",
    "    variants = variants_module.get_variants(log)\n",
    "    \n",
    "    # Calculate total number of cases in the log\n",
    "    total_cases = len(log)\n",
    "    \n",
    "    # Sort variants by frequency (descending)\n",
    "    sorted_variants = sorted(variants.items(), key=lambda x: -len(x[1]))\n",
    "    \n",
    "    # Determine dynamic coverage threshold\n",
    "    # For example: Use 80% coverage if the top variants dominate, else lower threshold\n",
    "    coverage_threshold = 0.8 if len(sorted_variants) <= 20 else 0.5\n",
    "    \n",
    "    # Calculate cumulative coverage and retain variants until the threshold is met\n",
    "    cumulative_coverage = 0\n",
    "    retained_variants = []\n",
    "    \n",
    "    for variant, cases in sorted_variants:\n",
    "        variant_coverage = len(cases) / total_cases\n",
    "        cumulative_coverage += variant_coverage\n",
    "        retained_variants.append(variant)\n",
    "        \n",
    "        if cumulative_coverage >= coverage_threshold:\n",
    "            break\n",
    "    \n",
    "    # Apply the filter to retain only the selected variants\n",
    "    filtered_log = variants_filter.apply(log, retained_variants)\n",
    "    \n",
    "    return filtered_log, coverage_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from standard to filtered 3x + props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping pdc2022_110010.xes - already processed\n",
      "Skipping pdc2022_000000.xes - already processed\n",
      "Skipping pdc2021_110111.xes - already processed\n",
      "Skipping pdc2024_101011.xes - already processed\n",
      "Skipping pdc_2020_0201110.xes - already processed\n",
      "Skipping pdc2024_121101.xes - already processed\n",
      "Skipping pdc_2020_1000111.xes - already processed\n",
      "Skipping pdc2021_011100.xes - already processed\n",
      "Skipping pdc2022_011101.xes - already processed\n",
      "Skipping pdc2021_001110.xes - already processed\n",
      "Skipping pdc2021_101111.xes - already processed\n",
      "Skipping pdc2021_001100.xes - already processed\n",
      "Skipping pdc_2016_7.xes - already processed\n",
      "Skipping pdc_2020_1201101.xes - already processed\n",
      "Skipping pdc_2020_1011101.xes - already processed\n",
      "Skipping pdc2022_111111.xes - already processed\n",
      "Skipping pdc2024_121001.xes - already processed\n",
      "Skipping pdc2022_011011.xes - already processed\n",
      "Skipping pdc2022_100010.xes - already processed\n",
      "Skipping pdc2022_121110.xes - already processed\n",
      "Skipping pdc2022_020100.xes - already processed\n",
      "Skipping pdc2024_001011.xes - already processed\n",
      "Skipping pdc2024_001110.xes - already processed\n",
      "Skipping pdc_2020_0210111.xes - already processed\n",
      "Skipping pdc_2020_0211010.xes - already processed\n",
      "Skipping pdc2024_120110.xes - already processed\n",
      "Skipping pdc2022_020001.xes - already processed\n",
      "Skipping pdc2021_101001.xes - already processed\n",
      "Skipping pdc_2017_8.xes - already processed\n",
      "Skipping pdc_2020_0201000.xes - already processed\n",
      "Skipping pdc2022_111110.xes - already processed\n",
      "Skipping pdc_2020_0200010.xes - already processed\n",
      "Skipping pdc2021_001111.xes - already processed\n",
      "Skipping pdc2022_021010.xes - already processed\n",
      "Skipping Experiment413.xes - already processed\n",
      "Skipping pdc_2017_1.xes - already processed\n",
      "Skipping Experiment422.xes - already processed\n",
      "Skipping pdc2022_001100.xes - already processed\n",
      "Skipping pdc2024_020001.xes - already processed\n",
      "Skipping pdc2021_111110.xes - already processed\n",
      "Skipping pdc_2020_1111010.xes - already processed\n",
      "Skipping pdc_2020_0100001.xes - already processed\n",
      "Skipping pdc2021_011101.xes - already processed\n",
      "Skipping pdc_2020_0010100.xes - already processed\n",
      "Skipping pdc_2020_1201110.xes - already processed\n",
      "Skipping pdc2024_100000.xes - already processed\n",
      "Skipping pdc2022_011110.xes - already processed\n",
      "Skipping pdc2024_101111.xes - already processed\n",
      "Skipping pdc2021_011010.xes - already processed\n",
      "Skipping pdc_2020_1010001.xes - already processed\n",
      "Skipping pdc2024_011100.xes - already processed\n",
      "Skipping Experiment623.xes - already processed\n",
      "Skipping pdc_2020_0011000.xes - already processed\n",
      "Skipping JUnit 4.12 Software Event Log.xes - already processed\n",
      "Skipping pdc_2020_1201000.xes - already processed\n",
      "Skipping pdc_2020_1010000.xes - already processed\n",
      "Skipping pdc2021_110100.xes - already processed\n",
      "Skipping pdc2024_010011.xes - already processed\n",
      "Skipping pdc2021_011011.xes - already processed\n",
      "Skipping pdc_2020_1210010.xes - already processed\n",
      "Skipping pdc2024_000110.xes - already processed\n",
      "Skipping pdc_2020_0200100.xes - already processed\n",
      "Skipping pdc2021_111100.xes - already processed\n",
      "Skipping pdc2024_101100.xes - already processed\n",
      "Skipping Experiment411.xes - already processed\n",
      "Skipping pdc2022_110110.xes - already processed\n",
      "Skipping pdc2024_010111.xes - already processed\n",
      "Skipping Experiment622.xes - already processed\n",
      "Skipping pdc2024_000111.xes - already processed\n",
      "Skipping pdc_2020_1210000.xes - already processed\n",
      "Skipping pdc_2017_3.xes - already processed\n",
      "Skipping pdc2022_000010.xes - already processed\n",
      "Skipping pdc2024_000000.xes - already processed\n",
      "Skipping pdc_2020_0111011.xes - already processed\n",
      "Skipping pdc_2017_4.xes - already processed\n",
      "Skipping pdc2022_010001.xes - already processed\n",
      "Skipping pdc2024_101001.xes - already processed\n",
      "Skipping pdc2021_110010.xes - already processed\n",
      "Skipping pdc_2020_1110100.xes - already processed\n",
      "Skipping pdc_2016_5.xes - already processed\n",
      "Skipping pdc_2020_1001111.xes - already processed\n",
      "Skipping pdc_2020_1010010.xes - already processed\n",
      "Skipping CreditRequirement.xes - already processed\n",
      "Skipping pdc2024_121000.xes - already processed\n",
      "Skipping Experiment621.xes - already processed\n",
      "Skipping pdc_2019_9.xes - already processed\n",
      "Skipping pdc_2016_8.xes - already processed\n",
      "Skipping pdc2021_101010.xes - already processed\n",
      "Skipping pdc_2020_0101100.xes - already processed\n",
      "Skipping pdc2021_000000.xes - already processed\n",
      "Skipping pdc2021_020110.xes - already processed\n",
      "Skipping pdc_2020_1111110.xes - already processed\n",
      "Skipping pdc2022_110101.xes - already processed\n",
      "Skipping pdc2022_100001.xes - already processed\n",
      "Skipping pdc_2020_1000011.xes - already processed\n",
      "Skipping pdc_2020_1011001.xes - already processed\n",
      "Skipping pdc2022_100011.xes - already processed\n",
      "Skipping pdc2024_021000.xes - already processed\n",
      "Skipping pdc_2019_8.xes - already processed\n",
      "Skipping pdc_2020_0100111.xes - already processed\n",
      "Skipping edited_hh102_weekends.xes - already processed\n",
      "Skipping pdc2024_011010.xes - already processed\n",
      "Skipping pdc2021_100100.xes - already processed\n",
      "Skipping pdc2024_111000.xes - already processed\n",
      "Skipping pdc_2020_0200110.xes - already processed\n",
      "Skipping pdc_2020_0201001.xes - already processed\n",
      "Skipping pdc2022_001010.xes - already processed\n",
      "Skipping pdc_2017_5.xes - already processed\n",
      "Skipping pdc_2020_1101000.xes - already processed\n",
      "Skipping pdc_2020_0110100.xes - already processed\n",
      "Skipping pdc2022_101100.xes - already processed\n",
      "Skipping pdc2024_111100.xes - already processed\n",
      "Skipping pdc_2020_0010111.xes - already processed\n",
      "Skipping pdc2021_010000.xes - already processed\n",
      "Skipping pdc2021_100101.xes - already processed\n",
      "Skipping Experiment531.xes - already processed\n",
      "Skipping pdc_2020_0111100.xes - already processed\n",
      "Skipping pdc_2020_1000010.xes - already processed\n",
      "Skipping pdc2024_120100.xes - already processed\n",
      "Skipping pdc_2020_1010111.xes - already processed\n",
      "Skipping pdc2024_010101.xes - already processed\n",
      "Skipping pdc2024_110000.xes - already processed\n",
      "Skipping pdc2024_101101.xes - already processed\n",
      "Skipping pdc_2020_1201010.xes - already processed\n",
      "Skipping pdc_2020_0011010.xes - already processed\n",
      "Skipping pdc2021_021011.xes - already processed\n",
      "Skipping pdc_2020_1200101.xes - already processed\n",
      "Skipping pdc2021_000001.xes - already processed\n",
      "Skipping pdc_2020_1211101.xes - already processed\n",
      "Skipping nasa-cev-1-10-splitted.xes - already processed\n",
      "Skipping pdc2024_021111.xes - already processed\n",
      "Skipping pdc2021_121100.xes - already processed\n",
      "Skipping pdc_2020_1201100.xes - already processed\n",
      "Skipping pdc2022_020111.xes - already processed\n",
      "Skipping Experiment522.xes - already processed\n",
      "Skipping pdc_2020_0111111.xes - already processed\n",
      "Skipping pdc_2020_0010000.xes - already processed\n",
      "Skipping pdc_2020_1011010.xes - already processed\n",
      "Skipping pdc2021_010010.xes - already processed\n",
      "Skipping pdc2021_110011.xes - already processed\n",
      "Skipping pdc_2020_1110010.xes - already processed\n",
      "Skipping pdc_2020_1111101.xes - already processed\n",
      "Skipping pdc2024_020101.xes - already processed\n",
      "Skipping pdc2022_010010.xes - already processed\n",
      "Skipping pdc2022_020011.xes - already processed\n",
      "Skipping pdc_2020_1211110.xes - already processed\n",
      "Skipping pdc2022_111001.xes - already processed\n",
      "Skipping pdc_2020_0000000.xes - already processed\n",
      "Skipping pdc2021_101101.xes - already processed\n",
      "Skipping pdc_2020_0210001.xes - already processed\n",
      "Skipping pdc2021_120110.xes - already processed\n",
      "Skipping RTFMP.xes - already processed\n",
      "Skipping pdc2021_020111.xes - already processed\n",
      "Skipping pdc_2020_0211101.xes - already processed\n",
      "Skipping pdc2024_020111.xes - already processed\n",
      "Skipping pdc_2020_1211111.xes - already processed\n",
      "Skipping SEPSIS.xes - already processed\n",
      "Skipping pdc_2020_1110000.xes - already processed\n",
      "Skipping pdc2022_121101.xes - already processed\n",
      "Skipping pdc_2016_2.xes - already processed\n",
      "Skipping pdc2022_110100.xes - already processed\n",
      "Skipping pdc2021_110001.xes - already processed\n",
      "Skipping pdc2022_100111.xes - already processed\n",
      "Skipping pdc2021_100110.xes - already processed\n",
      "Skipping pdc_2020_0001011.xes - already processed\n",
      "Skipping pdc_2020_0011011.xes - already processed\n",
      "Skipping pdc_2016_3.xes - already processed\n",
      "Skipping pdc2022_001001.xes - already processed\n",
      "Skipping pdc2024_020000.xes - already processed\n",
      "Skipping pdc_2020_0010110.xes - already processed\n",
      "Skipping pdc2024_101000.xes - already processed\n",
      "Skipping pdc2021_010100.xes - already processed\n",
      "Skipping pdc2024_021110.xes - already processed\n",
      "Skipping pdc_2020_0001000.xes - already processed\n",
      "Skipping pdc2024_000101.xes - already processed\n",
      "Skipping pdc_2020_0011110.xes - already processed\n",
      "Skipping pdc2021_111001.xes - already processed\n",
      "Skipping pdc_2020_0000010.xes - already processed\n",
      "Skipping pdc_2020_1111001.xes - already processed\n",
      "Skipping pdc2024_120000.xes - already processed\n",
      "Skipping pdc2021_020100.xes - already processed\n",
      "Skipping pdc_2020_0211100.xes - already processed\n",
      "Skipping Experiment632.xes - already processed\n",
      "Skipping pdc_2020_0000001.xes - already processed\n",
      "Skipping pdc2022_011100.xes - already processed\n",
      "Skipping pdc2021_000011.xes - already processed\n",
      "Skipping pdc2021_020101.xes - already processed\n",
      "Skipping pdc2024_100101.xes - already processed\n",
      "Skipping edited_hh110_labour.xes - already processed\n",
      "Skipping pdc2022_121111.xes - already processed\n",
      "Skipping pdc_2017_2.xes - already processed\n",
      "Skipping pdc_2020_0011111.xes - already processed\n",
      "Skipping pdc_2020_0210011.xes - already processed\n",
      "Skipping pdc2022_101101.xes - already processed\n",
      "Skipping pdc2024_021011.xes - already processed\n",
      "Skipping pdc2022_000001.xes - already processed\n",
      "Skipping pdc_2020_1000101.xes - already processed\n",
      "Skipping pdc2021_001010.xes - already processed\n",
      "Skipping pdc2024_000010.xes - already processed\n",
      "Skipping Experiment423.xes - already processed\n",
      "Skipping Experiment412.xes - already processed\n",
      "Skipping pdc2024_001000.xes - already processed\n",
      "Skipping pdc2024_121010.xes - already processed\n",
      "Skipping pdc_2020_1110110.xes - already processed\n",
      "Skipping pdc2021_121011.xes - already processed\n",
      "Skipping pdc_2020_1100000.xes - already processed\n",
      "Skipping pdc_2019_5.xes - already processed\n",
      "Skipping pdc_2020_1001101.xes - already processed\n",
      "Skipping pdc_2020_0101111.xes - already processed\n",
      "Skipping pdc_2020_1111111.xes - already processed\n",
      "Skipping pdc_2020_0100000.xes - already processed\n",
      "Skipping pdc2024_120111.xes - already processed\n",
      "Skipping pdc_2020_0200111.xes - already processed\n",
      "Skipping pdc2021_100000.xes - already processed\n",
      "Skipping pdc2024_121110.xes - already processed\n",
      "Skipping pdc2024_010010.xes - already processed\n",
      "Skipping pdc2024_001001.xes - already processed\n",
      "Skipping pdc2021_111000.xes - already processed\n",
      "Skipping pdc_2020_1211000.xes - already processed\n",
      "Skipping pdc_2020_0110101.xes - already processed\n",
      "Skipping pdc_2020_1100001.xes - already processed\n",
      "Skipping pdc2021_120000.xes - already processed\n",
      "Skipping pdc2022_121001.xes - already processed\n",
      "Skipping pdc2022_120000.xes - already processed\n",
      "Skipping pdc2021_101011.xes - already processed\n",
      "Skipping pdc2022_011001.xes - already processed\n",
      "Skipping pdc2022_010101.xes - already processed\n",
      "Skipping pdc2024_011101.xes - already processed\n",
      "Skipping pdc2021_011111.xes - already processed\n",
      "Skipping pdc2024_110010.xes - already processed\n",
      "Skipping pdc2022_120011.xes - already processed\n",
      "Skipping pdc2021_011110.xes - already processed\n",
      "Skipping pdc_2020_1001110.xes - already processed\n",
      "Skipping pdc2024_110101.xes - already processed\n",
      "Skipping pdc2022_121010.xes - already processed\n",
      "Skipping pdc_2020_1101001.xes - already processed\n",
      "Skipping pdc2024_010000.xes - already processed\n",
      "Skipping pdc_2020_0201010.xes - already processed\n",
      "Skipping pdc_2020_0200000.xes - already processed\n",
      "Skipping activitylog_uci_detailed_weekends.xes - already processed\n",
      "Skipping pdc2022_100101.xes - already processed\n",
      "Skipping pdc2021_010101.xes - already processed\n",
      "Skipping pdc2021_110110.xes - already processed\n",
      "Skipping pdc2024_100110.xes - already processed\n",
      "Skipping pdc_2020_1011000.xes - already processed\n",
      "Skipping pdc2021_001001.xes - already processed\n",
      "Skipping pdc2021_020011.xes - already processed\n",
      "Skipping pdc_2020_1000110.xes - already processed\n",
      "Skipping pdc_2020_0000110.xes - already processed\n",
      "Skipping pdc_2020_0101000.xes - already processed\n",
      "Skipping pdc_2020_1210001.xes - already processed\n",
      "Skipping pdc_2020_0110111.xes - already processed\n",
      "Skipping BPIC15_2f.xes - already processed\n",
      "Skipping pdc_2020_0010010.xes - already processed\n",
      "Skipping pdc2022_010000.xes - already processed\n",
      "Skipping pdc2022_001110.xes - already processed\n",
      "Skipping DomesticDeclarations.xes - already processed\n",
      "Skipping pdc2021_001000.xes - already processed\n",
      "Skipping pdc_2020_1211011.xes - already processed\n",
      "Skipping pdc2022_111000.xes - already processed\n",
      "Skipping pdc2022_100100.xes - already processed\n",
      "Skipping pdc_2020_1010011.xes - already processed\n",
      "Skipping pdc_2020_0100101.xes - already processed\n",
      "Skipping pdc2022_021100.xes - already processed\n",
      "Skipping pdc2024_020100.xes - already processed\n",
      "Skipping pdc_2020_1000100.xes - already processed\n",
      "Skipping ETM_Configuration1.xes - already processed\n",
      "Skipping pdc_2020_0000111.xes - already processed\n",
      "Skipping pdc_2020_0100010.xes - already processed\n",
      "Skipping pdc_2019_6.xes - already processed\n",
      "Skipping pdc_2020_0001100.xes - already processed\n",
      "Skipping pdc_2020_1210101.xes - already processed\n",
      "Skipping pdc_2020_0201101.xes - already processed\n",
      "Skipping pdc_2016_10.xes - already processed\n",
      "Skipping pdc2024_020010.xes - already processed\n",
      "Skipping pdc2022_111011.xes - already processed\n",
      "Skipping pdc_2020_1100111.xes - already processed\n",
      "Skipping pdc2024_120001.xes - already processed\n",
      "Skipping pdc_2016_1.xes - already processed\n",
      "Skipping activitylog_uci_detailed_labour.xes - already processed\n",
      "Skipping pdc_2017_7.xes - already processed\n",
      "Skipping pdc2022_001011.xes - already processed\n",
      "Skipping pdc2024_101010.xes - already processed\n",
      "Skipping pdc2022_101010.xes - already processed\n",
      "Skipping pdc2024_001010.xes - already processed\n",
      "Skipping edited_hh104_labour.xes - already processed\n",
      "Skipping PrepaidTravelCost.xes - already processed\n",
      "Skipping pdc2021_111010.xes - already processed\n",
      "Skipping pdc2021_100001.xes - already processed\n",
      "Skipping pdc2024_000100.xes - already processed\n",
      "Skipping pdc2024_100111.xes - already processed\n",
      "Skipping pdc2021_100111.xes - already processed\n",
      "Skipping pdc2021_021010.xes - already processed\n",
      "Skipping pdc2022_021001.xes - already processed\n",
      "Skipping pdc_2020_0201100.xes - already processed\n",
      "Skipping pdc_2020_0210010.xes - already processed\n",
      "Skipping pdc2021_120010.xes - already processed\n",
      "Skipping Experiment513.xes - already processed\n",
      "Skipping ETM_Configuration2.xes - already processed\n",
      "Skipping pdc2024_001100.xes - already processed\n",
      "Skipping Hospital Billing - Event Log.xes - already processed\n",
      "Skipping BPIC15_5f.xes - already processed\n",
      "Skipping pdc2024_110111.xes - already processed\n",
      "Skipping pdc2021_120100.xes - already processed\n",
      "Skipping pdc2024_111111.xes - already processed\n",
      "Skipping pdc_2020_0101001.xes - already processed\n",
      "Skipping pdc_2020_0101011.xes - already processed\n",
      "Skipping pdc_2020_0210110.xes - already processed\n",
      "Skipping pdc2021_120101.xes - already processed\n",
      "Skipping pdc2024_100100.xes - already processed\n",
      "Skipping pdc2021_100010.xes - already processed\n",
      "Skipping pdc2024_000001.xes - already processed\n",
      "Skipping pdc_2020_1100110.xes - already processed\n",
      "Skipping Experiment521.xes - already processed\n",
      "Skipping pdc_2020_1200100.xes - already processed\n",
      "Skipping pdc_2019_4.xes - already processed\n",
      "Skipping Experiment533.xes - already processed\n",
      "Skipping pdc2021_121101.xes - already processed\n",
      "Skipping pdc_2020_1001001.xes - already processed\n",
      "Skipping pdc_2020_1101110.xes - already processed\n",
      "Skipping pdc2022_101111.xes - already processed\n",
      "Skipping pdc2022_100000.xes - already processed\n",
      "Skipping pdc_2020_0211001.xes - already processed\n",
      "Skipping pdc2022_101001.xes - already processed\n",
      "Skipping BPIC14_f.xes - already processed\n",
      "Skipping pdc2024_110110.xes - already processed\n",
      "Skipping pdc_2020_1001000.xes - already processed\n",
      "Skipping pdc2022_101110.xes - already processed\n",
      "Skipping pdc2024_111001.xes - already processed\n",
      "Skipping pdc2024_011110.xes - already processed\n",
      "Skipping Experiment433.xes - already processed\n",
      "Skipping pdc_2020_0210100.xes - already processed\n",
      "Skipping pdc2022_000110.xes - already processed\n",
      "Skipping BPIC15_1f.xes - already processed\n",
      "Skipping pdc_2020_1210111.xes - already processed\n",
      "Skipping pdc_2020_0210101.xes - already processed\n",
      "Skipping pdc_2020_1101100.xes - already processed\n",
      "Skipping pdc_2020_1010101.xes - already processed\n",
      "Skipping pdc2021_101000.xes - already processed\n",
      "Skipping pdc_2020_0210000.xes - already processed\n",
      "Skipping pdc_2020_1200110.xes - already processed\n",
      "Skipping pdc_2020_1110111.xes - already processed\n",
      "Skipping pdc_2020_1111011.xes - already processed\n",
      "Skipping pdc2024_021010.xes - already processed\n",
      "Skipping Experiment633.xes - already processed\n",
      "Skipping pdc2021_021111.xes - already processed\n",
      "Skipping pdc_2019_2.xes - already processed\n",
      "Skipping pdc_2020_1210100.xes - already processed\n",
      "Skipping pdc2022_110000.xes - already processed\n",
      "Skipping pdc_2017_9.xes - already processed\n",
      "Skipping pdc2021_121111.xes - already processed\n",
      "Skipping pdc_2016_9.xes - already processed\n",
      "Skipping pdc2022_020101.xes - already processed\n",
      "Skipping pdc2024_100001.xes - already processed\n",
      "Skipping pdc_2020_1011011.xes - already processed\n",
      "Skipping pdc2022_020000.xes - already processed\n",
      "Skipping pdc_2020_1111000.xes - already processed\n",
      "Skipping pdc2024_021100.xes - already processed\n",
      "Skipping pdc_2020_0010011.xes - already processed\n",
      "Skipping pdc2022_000101.xes - already processed\n",
      "Skipping pdc_2020_1100010.xes - already processed\n",
      "Skipping pdc2021_121110.xes - already processed\n",
      "Skipping pdc_2020_0110110.xes - already processed\n",
      "Skipping pdc2022_000011.xes - already processed\n",
      "Skipping pdc2022_120101.xes - already processed\n",
      "Skipping pdc_2017_10.xes - already processed\n",
      "Skipping pdc2022_021110.xes - already processed\n",
      "Skipping pdc2024_121011.xes - already processed\n",
      "Skipping pdc2022_120100.xes - already processed\n",
      "Skipping pdc2022_100110.xes - already processed\n",
      "Skipping pdc2022_010111.xes - already processed\n",
      "Skipping pdc2021_120011.xes - already processed\n",
      "Skipping pdc2022_120111.xes - already processed\n",
      "Skipping Experiment431.xes - already processed\n",
      "Skipping pdc_2020_0101101.xes - already processed\n",
      "Skipping pdc_2020_1210110.xes - already processed\n",
      "Skipping pdc_2020_0211110.xes - already processed\n",
      "Skipping pdc2024_021101.xes - already processed\n",
      "Skipping pdc_2020_1110101.xes - already processed\n",
      "Skipping pdc_2020_1100101.xes - already processed\n",
      "Skipping pdc2022_010100.xes - already processed\n",
      "Skipping pdc_2020_1210011.xes - already processed\n",
      "Skipping pdc_2020_0111101.xes - already processed\n",
      "Skipping pdc2021_010110.xes - already processed\n",
      "Skipping pdc2021_000101.xes - already processed\n",
      "Skipping pdc2022_111100.xes - already processed\n",
      "Skipping pdc_2020_0110001.xes - already processed\n",
      "Skipping pdc_2020_1101010.xes - already processed\n",
      "Skipping pdc_2020_1001011.xes - already processed\n",
      "Skipping BPIC12.xes - already processed\n",
      "Skipping pdc2021_001101.xes - already processed\n",
      "Skipping edited_hh104_weekends.xes - already processed\n",
      "Skipping pdc2024_001101.xes - already processed\n",
      "Skipping pdc2022_110001.xes - already processed\n",
      "Skipping pdc2021_020000.xes - already processed\n",
      "Skipping pdc2024_120010.xes - already processed\n",
      "Skipping pdc_2020_0000101.xes - already processed\n",
      "Skipping pdc2024_021001.xes - already processed\n",
      "Skipping Experiment613.xes - already processed\n",
      "Skipping pdc_2020_0200011.xes - already processed\n",
      "Skipping pdc2022_111101.xes - already processed\n",
      "Skipping pdc2021_000110.xes - already processed\n",
      "Skipping pdc2022_110111.xes - already processed\n",
      "Skipping pdc_2020_0200001.xes - already processed\n",
      "Skipping pdc2024_020110.xes - already processed\n",
      "Skipping pdc_2020_0000011.xes - already processed\n",
      "Skipping pdc2024_110011.xes - already processed\n",
      "Skipping pdc2021_011001.xes - already processed\n",
      "Skipping pdc_2020_0100100.xes - already processed\n",
      "Skipping pdc2021_010001.xes - already processed\n",
      "Skipping pdc2022_111010.xes - already processed\n",
      "Skipping pdc2022_010110.xes - already processed\n",
      "Skipping pdc_2019_3.xes - already processed\n",
      "Skipping pdc_2020_1101111.xes - already processed\n",
      "Skipping pdc2024_010100.xes - already processed\n",
      "Skipping pdc2024_121100.xes - already processed\n",
      "Skipping pdc2022_000111.xes - already processed\n",
      "Skipping pdc_2020_1200011.xes - already processed\n",
      "Skipping pdc_2020_1200111.xes - already processed\n",
      "Skipping pdc_2017_6.xes - already processed\n",
      "Skipping pdc_2020_0011101.xes - already processed\n",
      "Skipping pdc2021_101100.xes - already processed\n",
      "Skipping pdc_2020_0101110.xes - already processed\n",
      "Skipping pdc_2020_1010110.xes - already processed\n",
      "Skipping pdc2024_020011.xes - already processed\n",
      "Skipping Experiment611.xes - already processed\n",
      "Skipping pdc_2020_0111010.xes - already processed\n",
      "Skipping pdc2024_110001.xes - already processed\n",
      "Skipping pdc_2020_0211000.xes - already processed\n",
      "Skipping pdc_2020_0001110.xes - already processed\n",
      "Skipping pdc2024_101110.xes - already processed\n",
      "Skipping pdc_2020_1100011.xes - already processed\n",
      "Skipping pdc_2020_1101011.xes - already processed\n",
      "Skipping pdc2024_121111.xes - already processed\n",
      "Skipping pdc2022_110011.xes - already processed\n",
      "Skipping pdc_2020_0100110.xes - already processed\n",
      "Skipping BPI Challenge 2017 - Offer log.xes - already processed\n",
      "Skipping BPI_Challenge_2013_incidents.xes - already processed\n",
      "Skipping pdc2021_121000.xes - already processed\n",
      "Skipping Experiment512.xes - already processed\n",
      "Skipping Experiment631.xes - already processed\n",
      "Skipping pdc2021_010011.xes - already processed\n",
      "Skipping pdc_2020_1001010.xes - already processed\n",
      "Skipping edited_hh102_labour.xes - already processed\n",
      "Skipping pdc2021_111011.xes - already processed\n",
      "Skipping pdc2021_021101.xes - already processed\n",
      "Skipping pdc2024_120101.xes - already processed\n",
      "Skipping pdc2022_120001.xes - already processed\n",
      "Skipping pdc2022_101011.xes - already processed\n",
      "Skipping Experiment612.xes - already processed\n",
      "Skipping pdc_2020_1201001.xes - already processed\n",
      "Skipping pdc2022_021101.xes - already processed\n",
      "Skipping pdc2024_001111.xes - already processed\n",
      "Skipping pdc_2020_1011100.xes - already processed\n",
      "Skipping pdc_2020_1010100.xes - already processed\n",
      "Skipping pdc2021_000010.xes - already processed\n",
      "Skipping pdc_2020_0000100.xes - already processed\n",
      "Skipping pdc_2020_1011111.xes - already processed\n",
      "Skipping pdc_2020_0111001.xes - already processed\n",
      "Skipping ETM_Configuration4.xes - already processed\n",
      "Skipping pdc2022_121000.xes - already processed\n",
      "Skipping pdc2024_120011.xes - already processed\n",
      "Skipping pdc2024_011111.xes - already processed\n",
      "Skipping pdc2024_111101.xes - already processed\n",
      "Skipping pdc2022_020110.xes - already processed\n",
      "Skipping pdc2021_120111.xes - already processed\n",
      "Skipping pdc_2016_6.xes - already processed\n",
      "Skipping pdc2021_100011.xes - already processed\n",
      "Skipping pdc2022_010011.xes - already processed\n",
      "Skipping pdc2024_010001.xes - already processed\n",
      "Skipping Experiment511.xes - already processed\n",
      "Skipping pdc2024_010110.xes - already processed\n",
      "Skipping pdc2022_021011.xes - already processed\n",
      "Skipping pdc2022_121011.xes - already processed\n",
      "Skipping pdc_2019_10.xes - already processed\n",
      "Skipping pdc2021_111101.xes - already processed\n",
      "Skipping pdc2022_020010.xes - already processed\n",
      "Skipping pdc2022_021111.xes - already processed\n",
      "Skipping pdc_2016_4.xes - already processed\n",
      "Skipping pdc_2020_1201011.xes - already processed\n",
      "Skipping pdc_2020_0010101.xes - already processed\n",
      "Skipping pdc_2020_0011001.xes - already processed\n",
      "Skipping pdc_2020_1100100.xes - already processed\n",
      "Skipping pdc2021_121010.xes - already processed\n",
      "Skipping pdc2021_110101.xes - already processed\n",
      "Skipping pdc_2020_0010001.xes - already processed\n",
      "Skipping ElectronicInvoicingENG.xes - already processed\n",
      "Skipping pdc2021_021100.xes - already processed\n",
      "Skipping pdc2024_111011.xes - already processed\n",
      "Skipping pdc_2020_0211111.xes - already processed\n",
      "Skipping pdc_2020_0011100.xes - already processed\n",
      "Skipping pdc_2019_7.xes - already processed\n",
      "Skipping pdc_2020_1000000.xes - already processed\n",
      "Skipping pdc_2020_0201011.xes - already processed\n",
      "Skipping pdc_2020_0111000.xes - already processed\n",
      "Skipping pdc2021_011000.xes - already processed\n",
      "Skipping pdc2021_021001.xes - already processed\n",
      "Skipping pdc2024_100010.xes - already processed\n",
      "Skipping pdc2024_011000.xes - already processed\n",
      "Skipping pdc2021_021000.xes - already processed\n",
      "Skipping pdc2024_000011.xes - already processed\n",
      "Skipping pdc_2020_1110011.xes - already processed\n",
      "Skipping pdc2022_121100.xes - already processed\n",
      "Skipping Experiment421.xes - already processed\n",
      "Skipping pdc2021_000100.xes - already processed\n",
      "Skipping pdc2021_010111.xes - already processed\n",
      "Skipping pdc2022_001111.xes - already processed\n",
      "Skipping pdc_2020_1200001.xes - already processed\n",
      "Skipping pdc2022_120110.xes - already processed\n",
      "Skipping pdc_2020_0201111.xes - already processed\n",
      "Skipping pdc2021_120001.xes - already processed\n",
      "Skipping pdc_2020_1110001.xes - already processed\n",
      "Skipping pdc2021_101110.xes - already processed\n",
      "Skipping pdc2024_100011.xes - already processed\n",
      "Skipping pdc_2020_0211011.xes - already processed\n",
      "Skipping pdc2021_121001.xes - already processed\n",
      "Skipping pdc2024_111110.xes - already processed\n",
      "Skipping pdc_2020_1211100.xes - already processed\n",
      "Skipping Experiment523.xes - already processed\n",
      "Skipping pdc_2020_1111100.xes - already processed\n",
      "Skipping pdc2022_021000.xes - already processed\n",
      "Skipping pdc2022_101000.xes - already processed\n",
      "Skipping pdc2021_020001.xes - already processed\n",
      "Skipping pdc2022_011010.xes - already processed\n",
      "Skipping pdc_2020_0101010.xes - already processed\n",
      "Skipping pdc_2020_1211010.xes - already processed\n",
      "Skipping pdc2022_000100.xes - already processed\n",
      "Skipping pdc2021_020010.xes - already processed\n",
      "Skipping pdc2021_000111.xes - already processed\n",
      "Skipping pdc_2020_0110000.xes - already processed\n",
      "Skipping pdc_2020_1201111.xes - already processed\n",
      "Skipping BPIC13_i.xes - already processed\n",
      "Skipping Experiment432.xes - already processed\n",
      "Skipping pdc2024_110100.xes - already processed\n",
      "Skipping pdc2022_011000.xes - already processed\n",
      "Skipping pdc_2020_0200101.xes - already processed\n",
      "Skipping pdc2022_001101.xes - already processed\n",
      "Skipping pdc_2020_0110010.xes - already processed\n",
      "Skipping pdc_2020_1101101.xes - already processed\n",
      "Skipping edited_hh110_weekends.xes - already processed\n",
      "Skipping pdc2021_021110.xes - already processed\n",
      "Skipping pdc_2020_1001100.xes - already processed\n",
      "Skipping pdc_2020_0111110.xes - already processed\n",
      "Skipping pdc_2020_1200010.xes - already processed\n",
      "Skipping pdc_2020_0110011.xes - already processed\n",
      "Skipping pdc_2020_0001111.xes - already processed\n",
      "Skipping Experiment532.xes - already processed\n",
      "Skipping pdc_2020_0001010.xes - already processed\n",
      "Skipping pdc_2020_1000001.xes - already processed\n",
      "Skipping pdc2022_120010.xes - already processed\n",
      "Skipping ETM_Configuration3.xes - already processed\n",
      "Skipping pdc2022_001000.xes - already processed\n",
      "Skipping pdc_2020_0001101.xes - already processed\n",
      "Skipping pdc2021_110000.xes - already processed\n",
      "Skipping pdc_2020_1211001.xes - already processed\n",
      "Skipping pdc_2019_1.xes - already processed\n",
      "Skipping pdc_2020_0100011.xes - already processed\n",
      "Skipping BPIC15_4f.xes - already processed\n",
      "Skipping pdc2021_001011.xes - already processed\n",
      "Skipping pdc2024_111010.xes - already processed\n",
      "Skipping pdc_2020_1011110.xes - already processed\n",
      "Skipping pdc_2020_0001001.xes - already processed\n",
      "Skipping pdc2022_011111.xes - already processed\n",
      "Skipping pdc2021_111111.xes - already processed\n"
     ]
    }
   ],
   "source": [
    "# Create output directories if they don't exist\n",
    "logs_path_parent = \"/home/jupyter-benjamin.andrick-3cf07/test/logs\"\n",
    "output_dirs = {\n",
    "    'case_size': os.path.join(logs_path_parent, 'case_size_filtered'),\n",
    "    'variants_top_k': os.path.join(logs_path_parent, 'variants_top_k_filtered'),\n",
    "    'variants_coverage': os.path.join(logs_path_parent, 'variants_coverage_filtered')\n",
    "}\n",
    "\n",
    "for directory in output_dirs.values():\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Initialize CSV files with headers\n",
    "csv_files = {\n",
    "    'case_size': os.path.join(output_dirs['case_size'], 'parameters.csv'),\n",
    "    'variants_top_k': os.path.join(output_dirs['variants_top_k'], 'parameters.csv'),\n",
    "    'variants_coverage': os.path.join(output_dirs['variants_coverage'], 'parameters.csv')\n",
    "}\n",
    "\n",
    "# Create CSV files with headers\n",
    "csv_headers = {\n",
    "    'case_size': ['filename', 'min_cases', 'max_cases'],\n",
    "    'variants_top_k': ['filename', 'top_k', 'coverage_threshold'],\n",
    "    'variants_coverage': ['filename', 'coverage_threshold']\n",
    "}\n",
    "\n",
    "props_csv_files = {\n",
    "    'case_size': os.path.join(output_dirs['case_size'], 'log_properties.csv'),\n",
    "    'variants_top_k': os.path.join(output_dirs['variants_top_k'], 'log_properties.csv'),\n",
    "    'variants_coverage': os.path.join(output_dirs['variants_coverage'], 'log_properties.csv')\n",
    "}\n",
    "\n",
    "for file_path in props_csv_files.values():\n",
    "    # The properties DataFrame will create its own headers when first saved\n",
    "    if not os.path.exists(file_path):\n",
    "        pd.DataFrame().to_csv(file_path, index=False)\n",
    "\n",
    "for file_path, headers in csv_headers.items():\n",
    "    with open(csv_files[file_path], 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "\n",
    "for file in os.listdir(log_path):\n",
    "    if file.endswith(\".xes\"):\n",
    "        # Check if the file has already been processed\n",
    "        case_size_path = os.path.join(output_dirs['case_size'], f\"{file}_case_size.xes\")\n",
    "        variants_top_k_path = os.path.join(output_dirs['variants_top_k'], f\"{file}_variants_top_k.xes\")\n",
    "        variants_coverage_path = os.path.join(output_dirs['variants_coverage'], f\"{file}_variants_coverage.xes\")\n",
    "        \n",
    "        if os.path.exists(case_size_path) and os.path.exists(variants_top_k_path) and os.path.exists(variants_coverage_path):\n",
    "            print(f\"Skipping {file} - already processed\")\n",
    "            continue\n",
    "        \n",
    "        print(file)\n",
    "        log = pm4py.read_xes(os.path.join(log_path, file))\n",
    "        df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "    \n",
    "        # Step 3: Add artificial timestamps if missing\n",
    "        if 'time:timestamp' not in df.columns:\n",
    "            df['time:timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='s')\n",
    "        \n",
    "        # Add case:concept:name if missing\n",
    "        if 'case:concept:name' not in df.columns:\n",
    "            # If case:id exists, use that\n",
    "            if 'case:id' in df.columns:\n",
    "                df['case:concept:name'] = df['case:id']\n",
    "            # Otherwise create sequential case IDs\n",
    "            else:\n",
    "                df['case:concept:name'] = df.index.astype(str)\n",
    "        \n",
    "        # Step 4: Convert back to event log\n",
    "        log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "        case_sizes = [len(trace) for trace in log]\n",
    "        min_cases = np.percentile(case_sizes, 20)\n",
    "        max_cases = np.percentile(case_sizes, 80)\n",
    "        filtered_log_case_size = filter_case_size(log, min_cases, max_cases)\n",
    "\n",
    "        variants = variants_module.get_variants(log)\n",
    "        coverage_threshold = 0.8\n",
    "        cumulative_coverage = 0\n",
    "        top_k = 0\n",
    "        # Modified sorting to handle list values by getting their length\n",
    "        for variant, count in sorted(variants.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "            # Adjust the coverage calculation to use the length of the list\n",
    "            cumulative_coverage += len(count) / len(log)\n",
    "            top_k += 1\n",
    "            if cumulative_coverage >= coverage_threshold:\n",
    "                break\n",
    "\n",
    "        #print(top_k)\n",
    "        filtered_log_variants_top_k = filter_variants_top_k(log, top_k)\n",
    "        filtered_log_variants_by_coverage_percentage, coverage_threshold = dynamic_filter_variants_by_coverage(log)\n",
    "        \n",
    "        pm4py.write_xes(filtered_log_case_size, case_size_path)\n",
    "        pm4py.write_xes(filtered_log_variants_top_k, variants_top_k_path)\n",
    "        pm4py.write_xes(filtered_log_variants_by_coverage_percentage, variants_coverage_path)\n",
    "\n",
    "        # Update CSV files with parameters\n",
    "        with open(csv_files['case_size'], 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([file, min_cases, max_cases])\n",
    "\n",
    "        with open(csv_files['variants_top_k'], 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([file, top_k, coverage_threshold])\n",
    "\n",
    "        with open(csv_files['variants_coverage'], 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([file, coverage_threshold])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories if they don't exist\n",
    "output_dirs = {\n",
    "    'case_size': os.path.join(log_path, 'case_size_filtered'),\n",
    "    'variants_top_k': os.path.join(log_path, 'variants_top_k_filtered'),\n",
    "    'variants_coverage': os.path.join(log_path, 'variants_coverage_filtered')\n",
    "}\n",
    "\n",
    "for directory in output_dirs.values():\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Initialize CSV files with headers\n",
    "csv_files = {\n",
    "    'case_size': os.path.join(output_dirs['case_size'], 'parameters.csv'),\n",
    "    'variants_top_k': os.path.join(output_dirs['variants_top_k'], 'parameters.csv'),\n",
    "    'variants_coverage': os.path.join(output_dirs['variants_coverage'], 'parameters.csv')\n",
    "}\n",
    "\n",
    "# Create CSV files with headers\n",
    "csv_headers = {\n",
    "    'case_size': ['filename', 'min_cases', 'max_cases'],\n",
    "    'variants_top_k': ['filename', 'top_k', 'coverage_threshold'],\n",
    "    'variants_coverage': ['filename', 'coverage_threshold']\n",
    "}\n",
    "\n",
    "props_csv_files = {\n",
    "    'case_size': os.path.join(output_dirs['case_size'], 'log_properties.csv'),\n",
    "    'variants_top_k': os.path.join(output_dirs['variants_top_k'], 'log_properties.csv'),\n",
    "    'variants_coverage': os.path.join(output_dirs['variants_coverage'], 'log_properties.csv')\n",
    "}\n",
    "\n",
    "for file_path in props_csv_files.values():\n",
    "    # The properties DataFrame will create its own headers when first saved\n",
    "    if not os.path.exists(file_path):\n",
    "        pd.DataFrame().to_csv(file_path, index=False)\n",
    "\n",
    "for file_path, headers in csv_headers.items():\n",
    "    with open(csv_files[file_path], 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "\n",
    "for file in os.listdir(log_path):\n",
    "    if file.endswith(\".xes\"):\n",
    "        print(file)\n",
    "        log = pm4py.read_xes(os.path.join(log_path, file))\n",
    "        df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "    \n",
    "        # Step 3: Add artificial timestamps if missing\n",
    "        if 'time:timestamp' not in df.columns:\n",
    "            df['time:timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='s')\n",
    "        \n",
    "        # Add case:concept:name if missing\n",
    "        if 'case:concept:name' not in df.columns:\n",
    "            # If case:id exists, use that\n",
    "            if 'case:id' in df.columns:\n",
    "                df['case:concept:name'] = df['case:id']\n",
    "            # Otherwise create sequential case IDs\n",
    "            else:\n",
    "                df['case:concept:name'] = df.index.astype(str)\n",
    "        # Step 3: Add artificial timestamps if missing\n",
    "        if 'time:timestamp' not in df.columns:\n",
    "            df['time:timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='s')\n",
    "        \n",
    "        # Step 4: Convert back to event log\n",
    "        log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "        case_sizes = [len(trace) for trace in log]\n",
    "        min_cases = np.percentile(case_sizes, 20)\n",
    "        max_cases = np.percentile(case_sizes, 80)\n",
    "        filtered_log_case_size = filter_case_size(log, min_cases, max_cases)\n",
    "\n",
    "        variants = variants_module.get_variants(log)\n",
    "        coverage_threshold = 0.8\n",
    "        cumulative_coverage = 0\n",
    "        top_k = 0\n",
    "        # Modified sorting to handle list values by getting their length\n",
    "        for variant, count in sorted(variants.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "            # Adjust the coverage calculation to use the length of the list\n",
    "            cumulative_coverage += len(count) / len(log)\n",
    "            top_k += 1\n",
    "            if cumulative_coverage >= coverage_threshold:\n",
    "                break\n",
    "\n",
    "        # ... rest of the code ...\n",
    "        print(top_k)\n",
    "        filtered_log_variants_top_k = filter_variants_top_k(log, top_k)\n",
    "        filtered_log_variants_by_coverage_percentage, coverage_threshold = dynamic_filter_variants_by_coverage(log)\n",
    "        \n",
    "        case_size_path = os.path.join(output_dirs['case_size'], f\"{file}_case_size.xes\")\n",
    "        variants_top_k_path = os.path.join(output_dirs['variants_top_k'], f\"{file}_variants_top_k.xes\")\n",
    "        variants_coverage_path = os.path.join(output_dirs['variants_coverage'], f\"{file}_variants_coverage.xes\")\n",
    "\n",
    "        pm4py.write_xes(filtered_log_case_size, case_size_path)\n",
    "        pm4py.write_xes(filtered_log_variants_top_k, variants_top_k_path)\n",
    "        pm4py.write_xes(filtered_log_variants_by_coverage_percentage, variants_coverage_path)\n",
    "\n",
    "        # Calculate and save properties for each filtered log\n",
    "        # # Calculate and save properties for each filtered log\n",
    "        # try:\n",
    "        #     props_case_size = calcEventLogPs.calculate_event_log_ps(filtered_log_case_size)\n",
    "        #     # Convert dictionary to DataFrame\n",
    "        #     props_case_size = pd.DataFrame([props_case_size])\n",
    "        # except Exception as e:\n",
    "        #     # Create DataFrame with error message\n",
    "        #     props_case_size = pd.DataFrame([{\n",
    "        #         'error': f\"Failed to calculate properties: {str(e)}\",\n",
    "        #         'filename': f\"{file}_case_size.xes\"\n",
    "        #     }])\n",
    "\n",
    "        # try:\n",
    "        #     props_variants_top_k = calcEventLogPs.calculate_event_log_ps(filtered_log_variants_top_k)\n",
    "        #     props_variants_top_k = pd.DataFrame([props_variants_top_k])\n",
    "        # except Exception as e:\n",
    "        #     props_variants_top_k = pd.DataFrame([{\n",
    "        #         'error': f\"Failed to calculate properties: {str(e)}\",\n",
    "        #         'filename': f\"{file}_variants_top_k.xes\"\n",
    "        #     }])\n",
    "\n",
    "        # try:\n",
    "        #     props_variants_coverage = calcEventLogPs.calculate_event_log_ps(filtered_log_variants_by_coverage_percentage)\n",
    "        #     props_variants_coverage = pd.DataFrame([props_variants_coverage])\n",
    "        # except Exception as e:\n",
    "        #     props_variants_coverage = pd.DataFrame([{\n",
    "        #         'error': f\"Failed to calculate properties: {str(e)}\",\n",
    "        #         'filename': f\"{file}_variants_coverage.xes\"\n",
    "        #     }])\n",
    "\n",
    "        # # Add filename column to each properties DataFrame (only if not already added in error case)\n",
    "        # if 'filename' not in props_case_size.columns:\n",
    "        #     props_case_size['filename'] = f\"{file}_case_size.xes\"\n",
    "        # if 'filename' not in props_variants_top_k.columns:\n",
    "        #     props_variants_top_k['filename'] = f\"{file}_variants_top_k.xes\"\n",
    "        # if 'filename' not in props_variants_coverage.columns:\n",
    "        #     props_variants_coverage['filename'] = f\"{file}_variants_coverage.xes\"\n",
    "\n",
    "        # # Append properties to respective CSV files\n",
    "        # props_case_size.to_csv(props_csv_files['case_size'], mode='a', header=False, index=False)\n",
    "        # props_variants_top_k.to_csv(props_csv_files['variants_top_k'], mode='a', header=False, index=False)\n",
    "        # props_variants_coverage.to_csv(props_csv_files['variants_coverage'], mode='a', header=False, index=False)# Update CSV files with parameters\n",
    "        with open(csv_files['case_size'], 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([file, min_cases, max_cases])\n",
    "\n",
    "        with open(csv_files['variants_top_k'], 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([file, top_k, coverage_threshold])\n",
    "\n",
    "        with open(csv_files['variants_coverage'], 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([file, coverage_threshold])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from standard to  3x filtered (combined_filtered) + props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directory\n",
    "output_dir = os.path.join(\"/home/jupyter-benjamin.andrick-3cf07/test/logs\", 'combined_filters')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize CSV files\n",
    "params_csv = os.path.join(output_dir, 'parameters.csv')\n",
    "props_csv = os.path.join(output_dir, 'log_properties.csv')\n",
    "\n",
    "# Create or load parameters CSV\n",
    "if os.path.exists(params_csv):\n",
    "    # Read existing parameters to skip processed files\n",
    "    processed_files = pd.read_csv(params_csv)['filename'].tolist()\n",
    "else:\n",
    "    # Create new parameters CSV with headers\n",
    "    processed_files = []\n",
    "    with open(params_csv, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['filename', 'min_cases', 'max_cases', 'top_k', 'coverage_threshold'])\n",
    "\n",
    "# Initialize properties CSV headers if needed\n",
    "props_headers_initialized = os.path.exists(props_csv)\n",
    "\n",
    "def preprocess_log(df):\n",
    "    \"\"\"Ensure required columns exist in the log DataFrame\"\"\"\n",
    "    if 'time:timestamp' not in df.columns:\n",
    "        df['time:timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='s')\n",
    "    \n",
    "    if 'case:concept:name' not in df.columns:\n",
    "        df['case:concept:name'] = df['case:id'] if 'case:id' in df.columns else df.index.astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each XES file\n",
    "for file in os.listdir(log_path):\n",
    "    if not file.endswith(\".xes\"):\n",
    "        continue\n",
    "        \n",
    "    # Skip if file was already processed\n",
    "    if file in processed_files:\n",
    "        print(f\"Skipping {file} - already processed\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {file}...\")\n",
    "        \n",
    "        # Read and preprocess log\n",
    "        log = pm4py.read_xes(os.path.join(log_path, file))\n",
    "        df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "        df = preprocess_log(df)\n",
    "        log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "        # Apply filters sequentially\n",
    "        print('Applying filters...')\n",
    "        # 1. Case size filter\n",
    "        case_sizes = [len(trace) for trace in log]\n",
    "        min_cases = np.percentile(case_sizes, 20)\n",
    "        max_cases = np.percentile(case_sizes, 80)\n",
    "        filtered_log = filter_case_size(log, min_cases, max_cases)\n",
    "\n",
    "        # 2. Top-k variants filter\n",
    "        variants = variants_module.get_variants(log)\n",
    "        coverage_threshold = 0.8\n",
    "        cumulative_coverage = 0\n",
    "        top_k = 0\n",
    "        for variant, cases in sorted(variants.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "            cumulative_coverage += len(cases) / len(log)\n",
    "            top_k += 1\n",
    "            if cumulative_coverage >= coverage_threshold:\n",
    "                break\n",
    "        filtered_log = filter_variants_top_k(filtered_log, top_k)\n",
    "\n",
    "        # 3. Dynamic coverage filter\n",
    "        filtered_log, final_coverage_threshold = dynamic_filter_variants_by_coverage(filtered_log)\n",
    "        \n",
    "        # Save results\n",
    "        print('Saving results...')\n",
    "        output_path = os.path.join(output_dir, f\"{file}_combined.xes\")\n",
    "        pm4py.write_xes(filtered_log, output_path)\n",
    "        continue\n",
    "        # Calculate and save properties\n",
    "        try:\n",
    "            props = calcEventLogPs.calculate_event_log_ps(filtered_log)\n",
    "            props['filename'] = f\"{file}_combined.xes\"\n",
    "            props_df = pd.DataFrame([props])\n",
    "            \n",
    "            # Initialize headers if this is the first successful calculation\n",
    "            if not props_headers_initialized:\n",
    "                props_df.to_csv(props_csv, index=False)\n",
    "                props_headers_initialized = True\n",
    "            else:\n",
    "                props_df.to_csv(props_csv, mode='a', header=False, index=False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating properties for {file}: {e}\")\n",
    "            error_df = pd.DataFrame([{\n",
    "                'error': str(e),\n",
    "                'filename': f\"{file}_combined.xes\"\n",
    "            }])\n",
    "            error_df.to_csv(props_csv, mode='a', header=not props_headers_initialized, index=False)\n",
    "            props_headers_initialized = True\n",
    "\n",
    "        # Save parameters\n",
    "        with open(params_csv, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([file, min_cases, max_cases, top_k, final_coverage_threshold])\n",
    "\n",
    "        # Add to processed files list\n",
    "        processed_files.append(file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtered logs + inductive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_log(df):\n",
    "    \"\"\"Ensure required columns exist in the log DataFrame\"\"\"\n",
    "    if 'time:timestamp' not in df.columns:\n",
    "        df['time:timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='s')\n",
    "    \n",
    "    if 'case:concept:name' not in df.columns:\n",
    "        df['case:concept:name'] = df['case:id'] if 'case:id' in df.columns else df.index.astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_xes_file_with_timeout(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "    df = preprocess_log(df)\n",
    "    log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "    \n",
    "    # Perform inductive mining\n",
    "    pt = inductive_miner.apply(log)\n",
    "    inductive_log = pm4py.objects.process_tree.semantics.generate_log(pt)\n",
    "    return inductive_log\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_timeout(file_path):\n",
    "    def thread_function(future, file_path):\n",
    "        try:\n",
    "            result = process_xes_file_with_timeout(file_path)\n",
    "            future.set_result(result)\n",
    "        except Exception as e:\n",
    "            future.set_exception(e)\n",
    "\n",
    "    future = concurrent.futures.Future()\n",
    "    thread = threading.Thread(target=thread_function, args=(future, file_path))\n",
    "    # Set thread as daemon so it will be terminated when main thread exits\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "\n",
    "    try:\n",
    "        return future.result(timeout=60*12)  \n",
    "    except concurrent.futures.TimeoutError:\n",
    "        print(f\"Processing timed out for {os.path.basename(file_path)}\")\n",
    "        # Add the file to timeout_fails.txt\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        timeout_file = os.path.join(folder_path, 'timeout_fails.txt')\n",
    "        with open(timeout_file, 'a') as f:\n",
    "            f.write(os.path.basename(file_path) + '\\n')\n",
    "        return None\n",
    "    finally:\n",
    "        # More aggressive thread termination\n",
    "        try:\n",
    "            thread.join(timeout=1)  # Give the thread 1 second to finish\n",
    "            if thread.is_alive():\n",
    "                print(f\"Thread for {os.path.basename(file_path)} did not terminate gracefully\")\n",
    "                # Attempt to raise exception in thread to force termination\n",
    "                import ctypes\n",
    "                thread_id = thread.ident\n",
    "                if thread_id is not None:\n",
    "                    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(\n",
    "                        ctypes.c_long(thread_id), \n",
    "                        ctypes.py_object(SystemExit)\n",
    "                    )\n",
    "                    if res > 1:\n",
    "                        # If it went wrong, cancel it\n",
    "                        ctypes.pythonapi.PyThreadState_SetAsyncExc(\n",
    "                            ctypes.c_long(thread_id), \n",
    "                            None\n",
    "                        )\n",
    "                # Give it a moment to terminate\n",
    "                thread.join(0.1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while terminating thread: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping pdc2024_110101.xes_combined.xes - already processed\n",
      "Skipping pdc2021_011011.xes_combined.xes - already processed\n",
      "Skipping pdc2022_121000.xes_combined.xes - already processed\n",
      "Skipping pdc_2020_0011010.xes_combined.xes - already processed\n",
      "Skipping pdc2024_100111.xes_combined.xes - already processed\n",
      "Skipping pdc2022_020110.xes_combined.xes - already processed\n",
      "Skipping pdc2024_120111.xes_combined.xes - already processed\n",
      "Skipping pdc2022_020010.xes_combined.xes - already processed\n",
      "Skipping pdc2024_120110.xes_combined.xes - already processed\n",
      "Skipping pdc2021_001101.xes_combined.xes - already processed\n",
      "Skipping pdc_2020_1211110.xes_combined.xes - already processed\n",
      "Skipping pdc_2020_1210001.xes_combined.xes - already processed\n",
      "Skipping pdc_2020_1211111.xes_combined.xes - already processed\n",
      "Skipping pdc2024_120101.xes_combined.xes - already processed\n",
      "Skipping pdc2021_121110.xes_combined.xes - already processed\n",
      "Skipping pdc2024_001000.xes_combined.xes - already processed\n",
      "Skipping pdc2021_121111.xes_combined.xes - already processed\n",
      "Skipping pdc_2020_0110100.xes_combined.xes - already processed\n",
      "Skipping pdc_2020_1111110.xes_combined.xes - already processed\n",
      "Processing Experiment633.xes_combined.xes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|| 449/449 [00:00<00:00, 1102.44it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(\"/home/jupyter-benjamin.andrick-3cf07/test/logs\", 'combined_ind')\n",
    "#os.makedirs(output_dir, exist_ok=True)\n",
    "log_path = \"/home/jupyter-benjamin.andrick-3cf07/test/logs/combined_filters\"\n",
    "# Process each XES file\n",
    "for file in os.listdir(log_path):\n",
    "    if not file.endswith(\".xes\"):\n",
    "        continue\n",
    "        \n",
    "    # Check if output file already exists\n",
    "    output_file = f\"{file}_inductive.xes\"\n",
    "    output_path = os.path.join(output_dir, output_file)\n",
    "    \n",
    "    # Skip if file was already processed\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Skipping {file} - already processed\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {file}...\")\n",
    "        \n",
    "        # Read and preprocess log\n",
    "        # log = pm4py.read_xes(os.path.join(log_path, file))\n",
    "        # df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "        # df = preprocess_log(df)\n",
    "        # log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "        \n",
    "        # # Perform inductive mining\n",
    "        # pt = inductive_miner.apply(log)\n",
    "        # inductive_log = pm4py.objects.process_tree.semantics.generate_log(pt)\n",
    "        inductive_log = run_with_timeout(os.path.join(log_path, file))\n",
    "        # Save the processed log\n",
    "        pm4py.write_xes(inductive_log, output_path)\n",
    "        print(f\"Saved processed log to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directory\n",
    "output_dir = os.path.join(\"/home/jupyter-benjamin.andrick-3cf07/test/logs\", 'inductive_logs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize CSV files\n",
    "props_csv = os.path.join(output_dir, 'log_properties.csv')\n",
    "log_path = \"/home/jupyter-benjamin.andrick-3cf07/test/logs/combined_filters\"\n",
    "# Initialize properties CSV headers if needed\n",
    "props_headers_initialized = os.path.exists(props_csv)\n",
    "\n",
    "def preprocess_log(df):\n",
    "    \"\"\"Ensure required columns exist in the log DataFrame\"\"\"\n",
    "    if 'time:timestamp' not in df.columns:\n",
    "        df['time:timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='s')\n",
    "    \n",
    "    if 'case:concept:name' not in df.columns:\n",
    "        df['case:concept:name'] = df['case:id'] if 'case:id' in df.columns else df.index.astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each XES file\n",
    "for file in os.listdir(log_path):\n",
    "    \n",
    "    if not file.endswith(\".xes\"):\n",
    "        continue\n",
    "        \n",
    "    # Skip if file was already processed\n",
    "    if file in processed_files:\n",
    "        print(f\"Skipping {file} - already processed\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {file}...\")\n",
    "        \n",
    "        # Read and preprocess log\n",
    "        log = pm4py.read_xes(os.path.join(log_path, file))\n",
    "        df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "        df = preprocess_log(df)\n",
    "        log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "        pt = inductive_miner.apply(log)\n",
    "        inductive_log = pm4py.objects.process_tree.semantics.generate_log(pt)\n",
    "        # Apply filters sequentially\n",
    "        \n",
    "        output_path = os.path.join(output_dir, f\"{file}_inductive.xes\")\n",
    "        pm4py.write_xes(inductive_log, output_path)\n",
    "        continue\n",
    "        # Calculate and save properties\n",
    "        try:\n",
    "            props = calcEventLogPs.calculate_event_log_ps(inductive_log)\n",
    "            props['filename'] = f\"{file}_inductive.xes\"\n",
    "            props_df = pd.DataFrame([props])\n",
    "            \n",
    "            # Initialize headers if this is the first successful calculation\n",
    "            if not props_headers_initialized:\n",
    "                props_df.to_csv(props_csv, index=False)\n",
    "                props_headers_initialized = True\n",
    "            else:\n",
    "                props_df.to_csv(props_csv, mode='a', header=False, index=False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating properties for {file}: {e}\")\n",
    "            error_df = pd.DataFrame([{\n",
    "                'error': str(e),\n",
    "                'filename': f\"{file}_inductive.xes\"\n",
    "            }])\n",
    "            error_df.to_csv(props_csv, mode='a', header=not props_headers_initialized, index=False)\n",
    "            props_headers_initialized = True\n",
    "\n",
    "        \n",
    "        # Add to processed files list\n",
    "        processed_files.append(file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calc properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pm4py\n",
    "import calcEventLogPs\n",
    "import pandas as pd\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "def calculate_properties_for_folder(input_path: str, output_csv: str) -> None:\n",
    "    \"\"\"\n",
    "    Calculate properties for all XES files in a folder and save them to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_path: Path to the folder containing XES files\n",
    "    - output_csv: Path where the CSV file should be saved\n",
    "    \"\"\"\n",
    "    all_properties = []\n",
    "    count = 1\n",
    "    # Iterate through all XES files in the folder\n",
    "    for file in os.listdir(input_path):\n",
    "        #count += 1\n",
    "       \n",
    "        if file.endswith(\".xes\"):\n",
    "            print(f\"Processing {file}...\")\n",
    "            try:\n",
    "                # Read the log and ensure proper conversion\n",
    "                log = pm4py.read_xes(os.path.join(input_path, file))\n",
    "                \n",
    "                # Convert to DataFrame and back to ensure proper format\n",
    "                df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "                \n",
    "                # Ensure required columns exist\n",
    "                if 'time:timestamp' not in df.columns:\n",
    "                    df['time:timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='s')\n",
    "                \n",
    "                if 'case:concept:name' not in df.columns:\n",
    "                    if 'case:id' in df.columns:\n",
    "                        df['case:concept:name'] = df['case:id']\n",
    "                    else:\n",
    "                        df['case:concept:name'] = df.index.astype(str)\n",
    "                \n",
    "                # Convert back to event log\n",
    "                log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "                \n",
    "                # Calculate properties\n",
    "                props = calcEventLogPs.calculate_event_log_ps(log)\n",
    "                props['filename'] = file\n",
    "                all_properties.append(props)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")  # Added for debugging\n",
    "                all_properties.append({\n",
    "                    'filename': file,\n",
    "                    'error': f\"Failed to calculate properties: {str(e)}\"\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame and save to CSV\n",
    "    df = pd.DataFrame(all_properties)\n",
    "    \n",
    "    # Move filename column to front\n",
    "    cols = ['filename'] + [col for col in df.columns if col != 'filename']\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Properties have been saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "output_file = \"/home/jupyter-benjamin.andrick-3cf07/test/logs/variants_coverage_filtered/log_properties.csv\"\n",
    "calculate_properties_for_folder(log_path, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
